<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Face Recognition Capture with Firebase</title>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <script type="module">
    import { initializeApp } from "https://www.gstatic.com/firebasejs/11.6.0/firebase-app.js";
    import { getDatabase, ref, set, get } from "https://www.gstatic.com/firebasejs/11.6.0/firebase-database.js";

    const firebaseConfig = {
      apiKey: "AIzaSyDsn16p7w8b-x8pEA_WTO5F2oUk4MHwcUc",
      authDomain: "wavetravel-3c4c4.firebaseapp.com",
      databaseURL: "https://wavetravel-3c4c4-default-rtdb.firebaseio.com",
      projectId: "wavetravel-3c4c4",
      storageBucket: "wavetravel-3c4c4.appspot.com",
      messagingSenderId: "298226098137",
      appId: "1:298226098137:web:720953b80e3e61c8bc43c8"
    };

    const app = initializeApp(firebaseConfig);
    const db = getDatabase(app);

    const MODEL_URL = 'https://raw.githubusercontent.com/myselfrakib/Face/main/models';

    const video = document.getElementById('videoInput');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');

    let knownDescriptors = [];
    let knownLabels = [];
    let nextId = 100;

    let currentDetections = [];

    function descriptorToArray(descriptor) {
      return Array.from(descriptor);
    }

    function arrayToDescriptor(arr) {
      return new Float32Array(arr);
    }

    async function loadKnownFaces() {
      const facesRef = ref(db, 'faces');
      const snapshot = await get(facesRef);
      knownDescriptors = [];
      knownLabels = [];
      nextId = 100;

      if (snapshot.exists()) {
        const data = snapshot.val();
        for (const [id, face] of Object.entries(data)) {
          if (face.encoding && face.name) {
            knownDescriptors.push(arrayToDescriptor(face.encoding));
            knownLabels.push(face.name);
            const numericId = parseInt(id, 10);
            if (!isNaN(numericId) && numericId >= nextId) nextId = numericId + 1;
          }
        }
      }
      console.log(`Loaded ${knownLabels.length} faces. Next ID: ${nextId}`);
    }

    async function saveFaceToFirebase(descriptor) {
      const id = nextId++;
      const name = `ticket${id - 99}`;
      const faceRef = ref(db, `faces/${id}`);
      await set(faceRef, {
        name,
        encoding: descriptorToArray(descriptor),
        timestamp: new Date().toISOString()
      });
      knownDescriptors.push(descriptor);
      knownLabels.push(name);
      console.log(`Saved new face as ${name} with ID ${id}`);
      alert(`Face saved as ${name}`);
    }

    async function startCamera() {
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
      ]);

      await loadKnownFaces();

      navigator.mediaDevices.getUserMedia({ video: {} })
        .then(stream => {
          video.srcObject = stream;
          video.play();
          video.addEventListener('play', () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            detectLoop();
          });
        })
        .catch(err => {
          console.error("Camera start failed:", err);
        });
    }

    async function detectLoop() {
      if (video.paused || video.ended) return;

      const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptors();

      currentDetections = detections;

      ctx.clearRect(0, 0, canvas.width, canvas.height);

      for (const detection of detections) {
        const box = detection.detection.box;
        ctx.strokeStyle = "blue";
        ctx.lineWidth = 2;
        ctx.strokeRect(box.x, box.y, box.width, box.height);
      }

      requestAnimationFrame(detectLoop);
    }

    document.getElementById('startBtn').addEventListener('click', () => {
      startCamera();
    });

    document.getElementById('captureBtn').addEventListener('click', async () => {
      if (!currentDetections.length) {
        alert('No face detected. Please make sure your face is visible.');
        return;
      }
      // Save the first detected face descriptor
      const descriptor = currentDetections[0].descriptor;
      await saveFaceToFirebase(descriptor);
    });
  </script>
  <style>
    body {
      background-color: #222;
      color: white;
      font-family: Arial, sans-serif;
      text-align: center;
      margin: 0; padding: 0;
      overflow: hidden;
    }
    #videoInput {
      margin-top: 10px;
      border-radius: 10px;
      max-width: 90vw;
      max-height: 60vh;
      transform: scaleX(-1);
    }
    #overlay {
      position: absolute;
      top: 60px;
      left: 50%;
      transform: translateX(-50%) scaleX(-1);
      pointer-events: none;
      border-radius: 10px;
      max-width: 90vw;
      max-height: 60vh;
    }
    button {
      margin: 15px 10px;
      padding: 10px 25px;
      font-size: 18px;
      border-radius: 8px;
      border: none;
      cursor: pointer;
      background: #08f;
      color: white;
      transition: background-color 0.3s;
    }
    button:hover {
      background: #06c;
    }
  </style>
</head>
<body>
  <h1>Face Recognition Capture</h1>
  <button id="startBtn">Start Camera</button>
  <button id="captureBtn">Capture Face</button><br/>
  <video id="videoInput" autoplay muted></video>
  <canvas id="overlay"></canvas>
</body>
</html>
