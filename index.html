<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Face Recognition with Firebase</title>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <script type="module">
    import { initializeApp } from "https://www.gstatic.com/firebasejs/11.6.0/firebase-app.js";
    import { getDatabase, ref, get, set, update } from "https://www.gstatic.com/firebasejs/11.6.0/firebase-database.js";

    // ----------- Firebase config --------------
    const firebaseConfig = {
      apiKey: "AIzaSyDsn16p7w8b-x8pEA_WTO5F2oUk4MHwcUc",
      authDomain: "wavetravel-3c4c4.firebaseapp.com",
      databaseURL: "https://wavetravel-3c4c4-default-rtdb.firebaseio.com",
      projectId: "wavetravel-3c4c4",
      storageBucket: "wavetravel-3c4c4.appspot.com",
      messagingSenderId: "298226098137",
      appId: "1:298226098137:web:720953b80e3e61c8bc43c8"
    };

    // Initialize Firebase
    const app = initializeApp(firebaseConfig);
    const db = getDatabase(app);

    // ----------- Variables --------------
    const MODEL_URL = 'https://raw.githubusercontent.com/myselfrakib/Face/main/models';
    const video = document.getElementById('videoInput');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');

    let knownDescriptors = [];
    let knownLabels = [];
    let nextId = 100;

    // Helper: convert Float32Array to normal array for Firebase saving
    function descriptorToArray(descriptor) {
      return Array.from(descriptor);
    }

    // Helper: convert array from Firebase back to Float32Array
    function arrayToDescriptor(arr) {
      return new Float32Array(arr);
    }

    // Load known faces from Firebase
    async function loadKnownFaces() {
      const facesRef = ref(db, 'faces');
      const snapshot = await get(facesRef);
      knownDescriptors = [];
      knownLabels = [];
      nextId = 100; // reset to 100 or you can find max ID from data

      if (snapshot.exists()) {
        const data = snapshot.val();
        for (const [id, face] of Object.entries(data)) {
          if (face.encoding && face.name) {
            knownDescriptors.push(arrayToDescriptor(face.encoding));
            knownLabels.push(face.name);
            const numericId = parseInt(id, 10);
            if (!isNaN(numericId) && numericId >= nextId) nextId = numericId + 1;
          }
        }
      }
      console.log(`Loaded ${knownLabels.length} faces. Next ID: ${nextId}`);
    }

    // Save unknown face to Firebase with generated ticket name
    async function saveUnknownFace(descriptor) {
      const id = nextId++;
      const name = `ticket${id - 99}`; // ticket1 for id=100 etc.
      const faceRef = ref(db, `faces/${id}`);
      await set(faceRef, {
        name,
        encoding: descriptorToArray(descriptor),
        timestamp: new Date().toISOString()
      });
      knownDescriptors.push(descriptor);
      knownLabels.push(name);
      console.log(`Saved new face as ${name} with ID ${id}`);
      return name;
    }

    async function setup() {
      // Load models from your GitHub repo
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
      ]);
      await loadKnownFaces();

      // Start webcam
      navigator.mediaDevices.getUserMedia({ video: {} })
        .then(stream => {
          video.srcObject = stream;
          video.play();
        })
        .catch(err => {
          console.error("Failed to access webcam:", err);
        });

      video.addEventListener('play', () => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        recognizeLoop();
      });
    }

    async function recognizeLoop() {
      if (video.paused || video.ended) return;

      // Detect faces with tinyFaceDetector for speed
      const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptors();

      ctx.clearRect(0, 0, canvas.width, canvas.height);

      for (const detection of detections) {
        const box = detection.detection.box;
        const descriptor = detection.descriptor;

        // Compare with known faces
        let bestMatch = { distance: 1.0, label: "Unknown" };
        for (let i = 0; i < knownDescriptors.length; i++) {
          const dist = faceapi.euclideanDistance(descriptor, knownDescriptors[i]);
          if (dist < bestMatch.distance) {
            bestMatch = { distance: dist, label: knownLabels[i] };
          }
        }

        // Threshold for matching
        const threshold = 0.5;
        let displayName = bestMatch.distance < threshold ? bestMatch.label : null;

        if (!displayName) {
          // Save unknown face
          displayName = await saveUnknownFace(descriptor);
        }

        // Draw box and label
        ctx.strokeStyle = displayName === "Unknown" ? "red" : "green";
        ctx.lineWidth = 2;
        ctx.strokeRect(box.x, box.y, box.width, box.height);
        ctx.fillStyle = ctx.strokeStyle;
        ctx.font = "16px Arial";
        ctx.fillText(displayName, box.x, box.y > 20 ? box.y - 5 : box.y + 15);
      }

      requestAnimationFrame(recognizeLoop);
    }

    setup();
  </script>
  <style>
    body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      text-align: center;
      background: #222;
      color: white;
      font-family: Arial, sans-serif;
    }
    #videoInput {
      position: absolute;
      top: 0; left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
      transform: scaleX(-1); /* mirror */
    }
    #overlay {
      position: absolute;
      top: 0; left: 0;
      width: 100vw;
      height: 100vh;
      pointer-events: none;
      transform: scaleX(-1);
    }
  </style>
</head>
<body>
  <video id="videoInput" autoplay muted></video>
  <canvas id="overlay"></canvas>
</body>
</html>
